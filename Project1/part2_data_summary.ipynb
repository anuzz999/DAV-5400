{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc1f8463",
   "metadata": {},
   "source": [
    "# Part 2: Data Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8015cc3f",
   "metadata": {},
   "source": [
    "## Name : Anuj Kumar Shah"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a9b4a4",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning and Summarization\n",
    "\n",
    "In this section, we focus on cleaning and summarizing our dataset to prepare it for further analysis. To streamline various data preprocessing tasks, we utilize a custom Python module, DataLoader, which provides reusable methods that encapsulate various data cleaning functionalities, embodying the principles of modularity and code reusability.\n",
    "\n",
    "- First, we invoke the DataLoader to load and inspect a sample of our data, enabling us to understand its structure and content.\n",
    "\n",
    "- Next, we clean the column names, removing any square brackets and leading or trailing spaces, ensuring consistency and improving readability.\n",
    "\n",
    "- We further refine our data by converting specific columns to appropriate data types, such as datetime and numeric, facilitating easier analysis.\n",
    "\n",
    "- After meticulously cleaning and transforming the data, we conclude this part by saving the cleaned dataset into a CSV file, ensuring that our refined data is preserved for subsequent parts of our analysis and use cases.\n",
    "\n",
    "\n",
    "Throughout this process, our DataLoader module plays a crucial role, providing reusable methods that encapsulate various data cleaning functionalities, embodying the principles of modularity and code reusability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f68db70",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1 Loading and Inspecting Data\n",
    "#### 2.1.1 Use the DataLoader to load a sample of the data.\n",
    "\n",
    "In this step, we will read our dataset which is in text file. We will read each text file into a pandas DataFrame. This DataFrame will be like a database helping us to understand the structure of our data better.  Database are usually a two-dimensional, size-mutable, and potentially heterogeneous tabular data structure with labeled axes (rows and columns).\n",
    "\n",
    "#### Process:\n",
    "\n",
    "- we will initialize the dataloaded with the file path.\n",
    "- We will load and display sample data to check if our module is working correctly or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8c8c880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[QUOTE_UNIXTIME], [QUOTE_READTIME], [QUOTE_DATE], [QUOTE_TIME_HOURS], [UNDERLYING_LAST], [EXPIRE_DATE], [EXPIRE_UNIX], [DTE], [C_DELTA], [C_GAMMA], [C_VEGA], [C_THETA], [C_RHO], [C_IV], [C_VOLUME], [C_LAST], [C_SIZE], [C_BID], [C_ASK], [STRIKE], [P_BID], [P_ASK], [P_SIZE], [P_LAST], [P_DELTA], [P_GAMMA], [P_VEGA], [P_THETA], [P_RHO], [P_IV], [P_VOLUME], [STRIKE_DISTANCE], [STRIKE_DISTANCE_PCT]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1672779600, 2023-01-03 16:00, 2023-01-03, 16.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1672779600, 2023-01-03 16:00, 2023-01-03, 16.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1672779600, 2023-01-03 16:00, 2023-01-03, 16.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1672779600, 2023-01-03 16:00, 2023-01-03, 16.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1672779600, 2023-01-03 16:00, 2023-01-03, 16.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  [QUOTE_UNIXTIME], [QUOTE_READTIME], [QUOTE_DATE], [QUOTE_TIME_HOURS], [UNDERLYING_LAST], [EXPIRE_DATE], [EXPIRE_UNIX], [DTE], [C_DELTA], [C_GAMMA], [C_VEGA], [C_THETA], [C_RHO], [C_IV], [C_VOLUME], [C_LAST], [C_SIZE], [C_BID], [C_ASK], [STRIKE], [P_BID], [P_ASK], [P_SIZE], [P_LAST], [P_DELTA], [P_GAMMA], [P_VEGA], [P_THETA], [P_RHO], [P_IV], [P_VOLUME], [STRIKE_DISTANCE], [STRIKE_DISTANCE_PCT]\n",
       "0  1672779600, 2023-01-03 16:00, 2023-01-03, 16.0...                                                                                                                                                                                                                                                                                                                                                          \n",
       "1  1672779600, 2023-01-03 16:00, 2023-01-03, 16.0...                                                                                                                                                                                                                                                                                                                                                          \n",
       "2  1672779600, 2023-01-03 16:00, 2023-01-03, 16.0...                                                                                                                                                                                                                                                                                                                                                          \n",
       "3  1672779600, 2023-01-03 16:00, 2023-01-03, 16.0...                                                                                                                                                                                                                                                                                                                                                          \n",
       "4  1672779600, 2023-01-03 16:00, 2023-01-03, 16.0...                                                                                                                                                                                                                                                                                                                                                          "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from py1 import DataLoader  \n",
    "\n",
    "# Initialize DataLoader with the file path\n",
    "data_loader = DataLoader(\"data/spy_eod_202301.txt\")\n",
    "\n",
    "\n",
    "# Load and display sample data\n",
    "sample_data = data_loader.load_sample_data()\n",
    "sample_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2fa6ba",
   "metadata": {},
   "source": [
    "Since our module is working correctly , we will now initialize the DataLoaded with the file path.\n",
    "\n",
    "#### 2.1.2 Inspect the basic structure and attributes of the data.\n",
    "-   The 'load_sample_data' function in our python module is used to load the data. Even though our files are text files, this function is quite versatile and can read text files by specifying the appropriate delimiter.\n",
    "-   We to specify a delimiter, such as a comma which separates the values in your text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "680d291f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[QUOTE_UNIXTIME]</th>\n",
       "      <th>[QUOTE_READTIME]</th>\n",
       "      <th>[QUOTE_DATE]</th>\n",
       "      <th>[QUOTE_TIME_HOURS]</th>\n",
       "      <th>[UNDERLYING_LAST]</th>\n",
       "      <th>[EXPIRE_DATE]</th>\n",
       "      <th>[EXPIRE_UNIX]</th>\n",
       "      <th>[DTE]</th>\n",
       "      <th>[C_DELTA]</th>\n",
       "      <th>[C_GAMMA]</th>\n",
       "      <th>...</th>\n",
       "      <th>[P_LAST]</th>\n",
       "      <th>[P_DELTA]</th>\n",
       "      <th>[P_GAMMA]</th>\n",
       "      <th>[P_VEGA]</th>\n",
       "      <th>[P_THETA]</th>\n",
       "      <th>[P_RHO]</th>\n",
       "      <th>[P_IV]</th>\n",
       "      <th>[P_VOLUME]</th>\n",
       "      <th>[STRIKE_DISTANCE]</th>\n",
       "      <th>[STRIKE_DISTANCE_PCT]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1672779600</td>\n",
       "      <td>2023-01-03 16:00</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>16.0</td>\n",
       "      <td>380.82</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>1672779600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.96551</td>\n",
       "      <td>0.00562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.00075</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>0.00072</td>\n",
       "      <td>-0.00483</td>\n",
       "      <td>-0.00015</td>\n",
       "      <td>1.21005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.8</td>\n",
       "      <td>0.186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1672779600</td>\n",
       "      <td>2023-01-03 16:00</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>16.0</td>\n",
       "      <td>380.82</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>1672779600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.96015</td>\n",
       "      <td>0.00703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.00093</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.00104</td>\n",
       "      <td>-0.00487</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.99616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.8</td>\n",
       "      <td>0.160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1672779600</td>\n",
       "      <td>2023-01-03 16:00</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>16.0</td>\n",
       "      <td>380.82</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>1672779600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95788</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00140</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.00105</td>\n",
       "      <td>-0.00538</td>\n",
       "      <td>-0.00007</td>\n",
       "      <td>0.91199</td>\n",
       "      <td></td>\n",
       "      <td>56.8</td>\n",
       "      <td>0.149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1672779600</td>\n",
       "      <td>2023-01-03 16:00</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>16.0</td>\n",
       "      <td>380.82</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>1672779600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.96337</td>\n",
       "      <td>0.00810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.00081</td>\n",
       "      <td>0.00026</td>\n",
       "      <td>0.00115</td>\n",
       "      <td>-0.00500</td>\n",
       "      <td>-0.00048</td>\n",
       "      <td>0.89065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55.8</td>\n",
       "      <td>0.147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1672779600</td>\n",
       "      <td>2023-01-03 16:00</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>16.0</td>\n",
       "      <td>380.82</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>1672779600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95600</td>\n",
       "      <td>0.00817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.00119</td>\n",
       "      <td>0.00022</td>\n",
       "      <td>0.00043</td>\n",
       "      <td>-0.00533</td>\n",
       "      <td>-0.00044</td>\n",
       "      <td>0.87004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.8</td>\n",
       "      <td>0.144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   [QUOTE_UNIXTIME]   [QUOTE_READTIME]  [QUOTE_DATE]   [QUOTE_TIME_HOURS]  \\\n",
       "0        1672779600   2023-01-03 16:00    2023-01-03                 16.0   \n",
       "1        1672779600   2023-01-03 16:00    2023-01-03                 16.0   \n",
       "2        1672779600   2023-01-03 16:00    2023-01-03                 16.0   \n",
       "3        1672779600   2023-01-03 16:00    2023-01-03                 16.0   \n",
       "4        1672779600   2023-01-03 16:00    2023-01-03                 16.0   \n",
       "\n",
       "    [UNDERLYING_LAST]  [EXPIRE_DATE]   [EXPIRE_UNIX]   [DTE]   [C_DELTA]  \\\n",
       "0              380.82     2023-01-03      1672779600     0.0     0.96551   \n",
       "1              380.82     2023-01-03      1672779600     0.0     0.96015   \n",
       "2              380.82     2023-01-03      1672779600     0.0     0.95788   \n",
       "3              380.82     2023-01-03      1672779600     0.0     0.96337   \n",
       "4              380.82     2023-01-03      1672779600     0.0     0.95600   \n",
       "\n",
       "    [C_GAMMA]  ...   [P_LAST]   [P_DELTA]   [P_GAMMA]   [P_VEGA]  [P_THETA]  \\\n",
       "0     0.00562  ...       0.01    -0.00075     0.00015    0.00072   -0.00483   \n",
       "1     0.00703  ...       0.02    -0.00093     0.00025    0.00104   -0.00487   \n",
       "2     0.00778  ...       0.00    -0.00140     0.00020    0.00105   -0.00538   \n",
       "3     0.00810  ...       0.02    -0.00081     0.00026    0.00115   -0.00500   \n",
       "4     0.00817  ...       0.01    -0.00119     0.00022    0.00043   -0.00533   \n",
       "\n",
       "    [P_RHO]   [P_IV]   [P_VOLUME]   [STRIKE_DISTANCE]   [STRIKE_DISTANCE_PCT]  \n",
       "0  -0.00015  1.21005     0.000000                70.8                   0.186  \n",
       "1   0.00000  0.99616     0.000000                60.8                   0.160  \n",
       "2  -0.00007  0.91199                             56.8                   0.149  \n",
       "3  -0.00048  0.89065     0.000000                55.8                   0.147  \n",
       "4  -0.00044  0.87004     0.000000                54.8                   0.144  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing DataLoader with the file path\n",
    "data_loader = DataLoader(\"data/spy_eod_202301.txt\")\n",
    "\n",
    "# Using DataLoader to load and display data with comma as the delimiter\n",
    "sample_data_comma_delimited = data_loader.load_sample_data(delimiter=\",\")\n",
    "\n",
    "sample_data_comma_delimited.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd3a9a1",
   "metadata": {},
   "source": [
    "The data has been successfully loaded with each attribute recognized as a separate column. However, there are still square brackets (`[]`) surrounding the column names and some data values, which we might want to clean up for ease of access and analysis later.\n",
    "\n",
    "##### Observations:\n",
    "\n",
    "-   The dataset includes various attributes related to options trading, such as option prices, Greeks, implied volatility, and expiry details, among others.\n",
    "-   Each row seems to represent a unique option based on different attributes like the expiry date and strike price.\n",
    "\n",
    "### 2.2 Cleaning and Concatinating\n",
    "\n",
    "#### 2.2.1 Removing the square brackets from the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "872fc715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\Desktop\\Project1_EDA\\py1.py:66: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  data.columns = data.columns.str.replace('[', '').str.replace(']', '')\n",
      "c:\\Users\\Asus\\Desktop\\Project1_EDA\\py1.py:66: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  data.columns = data.columns.str.replace('[', '').str.replace(']', '')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Index(['QUOTE_UNIXTIME', ' QUOTE_READTIME', ' QUOTE_DATE', ' QUOTE_TIME_HOURS',\n",
       "        ' UNDERLYING_LAST', ' EXPIRE_DATE', ' EXPIRE_UNIX', ' DTE', ' C_DELTA',\n",
       "        ' C_GAMMA', ' C_VEGA', ' C_THETA', ' C_RHO', ' C_IV', ' C_VOLUME',\n",
       "        ' C_LAST', ' C_SIZE', ' C_BID', ' C_ASK', ' STRIKE', ' P_BID', ' P_ASK',\n",
       "        ' P_SIZE', ' P_LAST', ' P_DELTA', ' P_GAMMA', ' P_VEGA', ' P_THETA',\n",
       "        ' P_RHO', ' P_IV', ' P_VOLUME', ' STRIKE_DISTANCE',\n",
       "        ' STRIKE_DISTANCE_PCT'],\n",
       "       dtype='object'),\n",
       "    QUOTE_UNIXTIME     QUOTE_READTIME   QUOTE_DATE   QUOTE_TIME_HOURS  \\\n",
       " 0      1672779600   2023-01-03 16:00   2023-01-03               16.0   \n",
       " 1      1672779600   2023-01-03 16:00   2023-01-03               16.0   \n",
       " 2      1672779600   2023-01-03 16:00   2023-01-03               16.0   \n",
       " 3      1672779600   2023-01-03 16:00   2023-01-03               16.0   \n",
       " 4      1672779600   2023-01-03 16:00   2023-01-03               16.0   \n",
       " \n",
       "     UNDERLYING_LAST  EXPIRE_DATE   EXPIRE_UNIX   DTE   C_DELTA   C_GAMMA  ...  \\\n",
       " 0            380.82   2023-01-03    1672779600   0.0   0.96551   0.00562  ...   \n",
       " 1            380.82   2023-01-03    1672779600   0.0   0.96015   0.00703  ...   \n",
       " 2            380.82   2023-01-03    1672779600   0.0   0.95788   0.00778  ...   \n",
       " 3            380.82   2023-01-03    1672779600   0.0   0.96337   0.00810  ...   \n",
       " 4            380.82   2023-01-03    1672779600   0.0   0.95600   0.00817  ...   \n",
       " \n",
       "     P_LAST   P_DELTA   P_GAMMA   P_VEGA  P_THETA    P_RHO     P_IV   P_VOLUME  \\\n",
       " 0     0.01  -0.00075   0.00015  0.00072 -0.00483 -0.00015  1.21005   0.000000   \n",
       " 1     0.02  -0.00093   0.00025  0.00104 -0.00487  0.00000  0.99616   0.000000   \n",
       " 2     0.00  -0.00140   0.00020  0.00105 -0.00538 -0.00007  0.91199              \n",
       " 3     0.02  -0.00081   0.00026  0.00115 -0.00500 -0.00048  0.89065   0.000000   \n",
       " 4     0.01  -0.00119   0.00022  0.00043 -0.00533 -0.00044  0.87004   0.000000   \n",
       " \n",
       "     STRIKE_DISTANCE   STRIKE_DISTANCE_PCT  \n",
       " 0              70.8                 0.186  \n",
       " 1              60.8                 0.160  \n",
       " 2              56.8                 0.149  \n",
       " 3              55.8                 0.147  \n",
       " 4              54.8                 0.144  \n",
       " \n",
       " [5 rows x 33 columns])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using DataLoader to clean the column names\n",
    "cleaned_data = DataLoader.clean_column_names(sample_data_comma_delimited.copy())\n",
    "\n",
    "# Displaying the cleaned column names and the first few rows of the data\n",
    "cleaned_data.columns, cleaned_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6a737b",
   "metadata": {},
   "source": [
    "The square brackets in the column names have been successfully removed. Here's a brief explanation of what was done:\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "-   Removing Square Brackets:\n",
    "    -   The column names originally had square brackets around them, like `[QUOTE_UNIXTIME]`. To clean this, the square brackets were removed by replacing them with empty strings.\n",
    "    -   A Python string method `.replace()` was used in  the module which was in combination with pandas functionality to apply this operation to all column names.\n",
    "\n",
    "#### Current State of the Data:\n",
    "\n",
    "-   Columns Cleaned:\n",
    "\n",
    "    -   The column names are now clean and easily accessible for future operations.\n",
    "-   Data Preview:\n",
    "\n",
    "    -   The data consists of various attributes like `QUOTE_UNIXTIME`, `QUOTE_READTIME`, Greeks such as `C_DELTA`, `C_GAMMA`, and others, and option prices such as `C_BID`, `C_ASK`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3979444c",
   "metadata": {},
   "source": [
    "#### 2.2.2 Load and Concatenate All Files\n",
    "\n",
    "- Load each text file and concatenate them into a single DataFrame for a comprehensive analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21ce02d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\Desktop\\Project_1_EDA\\py1.py:66: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  data.columns = data.columns.str.replace('[', '').str.replace(']', '')\n",
      "c:\\Users\\Asus\\Desktop\\Project_1_EDA\\py1.py:66: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  data.columns = data.columns.str.replace('[', '').str.replace(']', '')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((245695, 33),\n",
       "    QUOTE_UNIXTIME     QUOTE_READTIME   QUOTE_DATE   QUOTE_TIME_HOURS  \\\n",
       " 0      1672779600   2023-01-03 16:00   2023-01-03               16.0   \n",
       " 1      1672779600   2023-01-03 16:00   2023-01-03               16.0   \n",
       " 2      1672779600   2023-01-03 16:00   2023-01-03               16.0   \n",
       " 3      1672779600   2023-01-03 16:00   2023-01-03               16.0   \n",
       " 4      1672779600   2023-01-03 16:00   2023-01-03               16.0   \n",
       " \n",
       "     UNDERLYING_LAST  EXPIRE_DATE   EXPIRE_UNIX   DTE   C_DELTA   C_GAMMA  ...  \\\n",
       " 0            380.82   2023-01-03    1672779600   0.0   0.96551   0.00562  ...   \n",
       " 1            380.82   2023-01-03    1672779600   0.0   0.96015   0.00703  ...   \n",
       " 2            380.82   2023-01-03    1672779600   0.0   0.95788   0.00778  ...   \n",
       " 3            380.82   2023-01-03    1672779600   0.0   0.96337   0.00810  ...   \n",
       " 4            380.82   2023-01-03    1672779600   0.0   0.95600   0.00817  ...   \n",
       " \n",
       "     P_LAST   P_DELTA   P_GAMMA   P_VEGA  P_THETA    P_RHO       P_IV  \\\n",
       " 0     0.01  -0.00075   0.00015  0.00072 -0.00483 -0.00015   1.210050   \n",
       " 1     0.02  -0.00093   0.00025  0.00104 -0.00487  0.00000   0.996160   \n",
       " 2     0.00  -0.00140   0.00020  0.00105 -0.00538 -0.00007   0.911990   \n",
       " 3     0.02  -0.00081   0.00026  0.00115 -0.00500 -0.00048   0.890650   \n",
       " 4     0.01  -0.00119   0.00022  0.00043 -0.00533 -0.00044   0.870040   \n",
       " \n",
       "     P_VOLUME   STRIKE_DISTANCE   STRIKE_DISTANCE_PCT  \n",
       " 0   0.000000              70.8                 0.186  \n",
       " 1   0.000000              60.8                 0.160  \n",
       " 2                         56.8                 0.149  \n",
       " 3   0.000000              55.8                 0.147  \n",
       " 4   0.000000              54.8                 0.144  \n",
       " \n",
       " [5 rows x 33 columns])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the file paths\n",
    "file_paths = [\"data/spy_eod_202301.txt\", \n",
    "              \"data/spy_eod_202302.txt\", \n",
    "              \"data/spy_eod_202303.txt\"]\n",
    "\n",
    "# Assuming DataLoader has a method to load and concatenate files\n",
    "# Using DataLoader to load and concatenate all files into a single DataFrame\n",
    "all_data = DataLoader.load_and_concatenate_files(file_paths)\n",
    "\n",
    "# Using DataLoader to clean the column names (calling the method on the class, not the instance)\n",
    "all_data_cleaned = DataLoader.clean_column_names(all_data.copy())\n",
    "\n",
    "# Displaying the shape and the first few rows of the concatenated DataFrame\n",
    "all_data_cleaned.shape, all_data_cleaned.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9d7ed0",
   "metadata": {},
   "source": [
    "The data from all files has been successfully loaded and concatenated into a single DataFrame. Here's a summary of the above:\n",
    "\n",
    "#### i. Loading and Concatenating\n",
    "\n",
    "-   All text files have been read into individual DataFrames.\n",
    "-   These DataFrames have been concatenated, meaning they have been stacked on top of each other to form a single DataFrame. This will make the analysis easier as we have all the data in one place.\n",
    "-   The dataset now has 245,695 rows and 33 columns.\n",
    "\n",
    "#### ii. Cleaning Column Names\n",
    "\n",
    "-   The square brackets in the column names have been removed to simplify accessing the columns later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c24a1c",
   "metadata": {},
   "source": [
    "### 2.3. Removing Unnecessary Columns \n",
    "\n",
    "- This could include columns that are redundant, have too many missing values, or are not useful for the analysis.       \n",
    "- To simplify the dataset, we will be keeping only the essential columns relevant to our research questions.\n",
    "- We will also identify and remove columns that might not be crucial for answering the research questions.\n",
    "- This will make the dataset more manageable and easier to understand.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0cc2cb",
   "metadata": {},
   "source": [
    "#### 2.3.1 Checking Existing Columns\n",
    "- Here we Check the existing columns and clean it if required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9930b056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['QUOTE_UNIXTIME', ' QUOTE_READTIME', ' QUOTE_DATE', ' QUOTE_TIME_HOURS',\n",
      "       ' UNDERLYING_LAST', ' EXPIRE_DATE', ' EXPIRE_UNIX', ' DTE', ' C_DELTA',\n",
      "       ' C_GAMMA', ' C_VEGA', ' C_THETA', ' C_RHO', ' C_IV', ' C_VOLUME',\n",
      "       ' C_LAST', ' C_SIZE', ' C_BID', ' C_ASK', ' STRIKE', ' P_BID', ' P_ASK',\n",
      "       ' P_SIZE', ' P_LAST', ' P_DELTA', ' P_GAMMA', ' P_VEGA', ' P_THETA',\n",
      "       ' P_RHO', ' P_IV', ' P_VOLUME', ' STRIKE_DISTANCE',\n",
      "       ' STRIKE_DISTANCE_PCT'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(all_data_cleaned.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa33535",
   "metadata": {},
   "source": [
    "#### Correcting Column Names:\n",
    "\n",
    "It appears that there are extra spaces at the beginning of some column names. We will remove the leading spaces from the column names and then proceed to keep only the necessary columns.\n",
    "\n",
    "-   Removed leading spaces from the column names to correctly reference them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad61397d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['QUOTE_UNIXTIME', 'QUOTE_READTIME', 'QUOTE_DATE', 'QUOTE_TIME_HOURS',\n",
      "       'UNDERLYING_LAST', 'EXPIRE_DATE', 'EXPIRE_UNIX', 'DTE', 'C_DELTA',\n",
      "       'C_GAMMA', 'C_VEGA', 'C_THETA', 'C_RHO', 'C_IV', 'C_VOLUME', 'C_LAST',\n",
      "       'C_SIZE', 'C_BID', 'C_ASK', 'STRIKE', 'P_BID', 'P_ASK', 'P_SIZE',\n",
      "       'P_LAST', 'P_DELTA', 'P_GAMMA', 'P_VEGA', 'P_THETA', 'P_RHO', 'P_IV',\n",
      "       'P_VOLUME', 'STRIKE_DISTANCE', 'STRIKE_DISTANCE_PCT'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\Desktop\\Project_1_EDA\\py1.py:66: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  data.columns = data.columns.str.replace('[', '').str.replace(']', '')\n",
      "c:\\Users\\Asus\\Desktop\\Project_1_EDA\\py1.py:66: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  data.columns = data.columns.str.replace('[', '').str.replace(']', '')\n"
     ]
    }
   ],
   "source": [
    "# Using DataLoader to clean leading spaces in column names\n",
    "all_data_cleaned = DataLoader.clean_leading_spaces_in_columns(all_data_cleaned)\n",
    "\n",
    "# Using DataLoader to clean the square brackets in column names\n",
    "all_data_cleaned = DataLoader.clean_column_names(all_data_cleaned)\n",
    "\n",
    "# Displaying the cleaned column names\n",
    "print(all_data_cleaned.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140ed016",
   "metadata": {},
   "source": [
    "#### 2.3.2 Selecting Relevant Columns \n",
    "We'll select columns that are directly relevant to the research questions. Below are the columns which we are keeping:\n",
    "\n",
    "1.  `QUOTE_DATE`: Date of the quote - important for time series analysis.\n",
    "2.  `EXPIRE_DATE`: Expiry date of the option - crucial for analyzing time left until expiry.\n",
    "3.  `DTE`: Days till Expiry - useful for analyzing how option characteristics change over time.\n",
    "4.  `C_DELTA`, `C_GAMMA`, `C_VEGA`, `C_THETA`, `C_RHO`: Greeks for Call options - essential for analyzing option risks and exposures.\n",
    "5.  `C_IV`: Implied Volatility of Call options - key metric in option pricing.\n",
    "6.  `P_DELTA`, `P_GAMMA`, `P_VEGA`, `P_THETA`, `P_RHO`: Greeks for Put options.\n",
    "7.  `P_IV`: Implied Volatility of Put options.\n",
    "8.  `STRIKE_DISTANCE`: Distance between strike price and current underlying price - important for analyzing moneyness of options.\n",
    "\n",
    "We exclude columns related to bid, ask, last traded price, and volume, as our focus is more on the Greeks, implied volatility, and their relationship with days to expiry and strike distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05583051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QUOTE_DATE</th>\n",
       "      <th>EXPIRE_DATE</th>\n",
       "      <th>DTE</th>\n",
       "      <th>C_DELTA</th>\n",
       "      <th>C_GAMMA</th>\n",
       "      <th>C_VEGA</th>\n",
       "      <th>C_THETA</th>\n",
       "      <th>C_RHO</th>\n",
       "      <th>C_IV</th>\n",
       "      <th>P_DELTA</th>\n",
       "      <th>P_GAMMA</th>\n",
       "      <th>P_VEGA</th>\n",
       "      <th>P_THETA</th>\n",
       "      <th>P_RHO</th>\n",
       "      <th>P_IV</th>\n",
       "      <th>STRIKE_DISTANCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.96551</td>\n",
       "      <td>0.00562</td>\n",
       "      <td>0.00913</td>\n",
       "      <td>-0.10519</td>\n",
       "      <td>0.00095</td>\n",
       "      <td>4.346730</td>\n",
       "      <td>-0.00075</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>0.00072</td>\n",
       "      <td>-0.00483</td>\n",
       "      <td>-0.00015</td>\n",
       "      <td>1.210050</td>\n",
       "      <td>70.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.96015</td>\n",
       "      <td>0.00703</td>\n",
       "      <td>0.00997</td>\n",
       "      <td>-0.10512</td>\n",
       "      <td>0.00032</td>\n",
       "      <td>3.872190</td>\n",
       "      <td>-0.00093</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.00104</td>\n",
       "      <td>-0.00487</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.996160</td>\n",
       "      <td>60.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95788</td>\n",
       "      <td>0.00778</td>\n",
       "      <td>0.01014</td>\n",
       "      <td>-0.10536</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>3.682610</td>\n",
       "      <td>-0.00140</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.00105</td>\n",
       "      <td>-0.00538</td>\n",
       "      <td>-0.00007</td>\n",
       "      <td>0.911990</td>\n",
       "      <td>56.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.96337</td>\n",
       "      <td>0.00810</td>\n",
       "      <td>0.00901</td>\n",
       "      <td>-0.06979</td>\n",
       "      <td>0.00059</td>\n",
       "      <td>3.598520</td>\n",
       "      <td>-0.00081</td>\n",
       "      <td>0.00026</td>\n",
       "      <td>0.00115</td>\n",
       "      <td>-0.00500</td>\n",
       "      <td>-0.00048</td>\n",
       "      <td>0.890650</td>\n",
       "      <td>55.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95600</td>\n",
       "      <td>0.00817</td>\n",
       "      <td>0.01064</td>\n",
       "      <td>-0.10918</td>\n",
       "      <td>0.00021</td>\n",
       "      <td>3.590290</td>\n",
       "      <td>-0.00119</td>\n",
       "      <td>0.00022</td>\n",
       "      <td>0.00043</td>\n",
       "      <td>-0.00533</td>\n",
       "      <td>-0.00044</td>\n",
       "      <td>0.870040</td>\n",
       "      <td>54.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    QUOTE_DATE  EXPIRE_DATE  DTE  C_DELTA  C_GAMMA   C_VEGA  C_THETA    C_RHO  \\\n",
       "0   2023-01-03   2023-01-03  0.0  0.96551  0.00562  0.00913 -0.10519  0.00095   \n",
       "1   2023-01-03   2023-01-03  0.0  0.96015  0.00703  0.00997 -0.10512  0.00032   \n",
       "2   2023-01-03   2023-01-03  0.0  0.95788  0.00778  0.01014 -0.10536  0.00025   \n",
       "3   2023-01-03   2023-01-03  0.0  0.96337  0.00810  0.00901 -0.06979  0.00059   \n",
       "4   2023-01-03   2023-01-03  0.0  0.95600  0.00817  0.01064 -0.10918  0.00021   \n",
       "\n",
       "        C_IV  P_DELTA  P_GAMMA   P_VEGA  P_THETA    P_RHO       P_IV  \\\n",
       "0   4.346730 -0.00075  0.00015  0.00072 -0.00483 -0.00015   1.210050   \n",
       "1   3.872190 -0.00093  0.00025  0.00104 -0.00487  0.00000   0.996160   \n",
       "2   3.682610 -0.00140  0.00020  0.00105 -0.00538 -0.00007   0.911990   \n",
       "3   3.598520 -0.00081  0.00026  0.00115 -0.00500 -0.00048   0.890650   \n",
       "4   3.590290 -0.00119  0.00022  0.00043 -0.00533 -0.00044   0.870040   \n",
       "\n",
       "   STRIKE_DISTANCE  \n",
       "0             70.8  \n",
       "1             60.8  \n",
       "2             56.8  \n",
       "3             55.8  \n",
       "4             54.8  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns to keep based on their relevance to the research questions\n",
    "columns_to_keep = [\n",
    "    'QUOTE_DATE', 'EXPIRE_DATE', 'DTE', 'C_DELTA', 'C_GAMMA', 'C_VEGA', \n",
    "    'C_THETA', 'C_RHO', 'C_IV', 'P_DELTA', 'P_GAMMA', 'P_VEGA', \n",
    "    'P_THETA', 'P_RHO', 'P_IV', 'STRIKE_DISTANCE'\n",
    "]\n",
    "\n",
    "# Using DataLoader to clean leading spaces in column names\n",
    "all_data_cleaned = DataLoader.clean_leading_spaces_in_columns(all_data)\n",
    "\n",
    "# Using DataLoader to select specific columns\n",
    "selected_data = DataLoader.select_columns(all_data_cleaned, columns_to_keep)\n",
    "\n",
    "# Displaying the first few rows of the selected DataFrame\n",
    "selected_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ab8b89",
   "metadata": {},
   "source": [
    "The dataset has been successfully cleaned by keeping only the relevant columns.\n",
    "-   Kept only the columns that are directly relevant to the research questions. These columns include details like quote and expiry dates, Greeks for both call and put options, and implied volatilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842e66c5",
   "metadata": {},
   "source": [
    "####  Next Steps:\n",
    "We can proceed to handle missing values and perform other necessary data preprocessing steps as part of the data summary and cleaning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c300df1",
   "metadata": {},
   "source": [
    "### 2.4 Further Cleaning\n",
    "Next steps in data cleaning and summarization are:\n",
    "\n",
    "#### 2.4.1 Handling Missing Values\n",
    "\n",
    "-   Identify any columns or rows with missing values.\n",
    "-   Decide on a strategy to handle them, such as removing or imputing missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8774c93a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QUOTE_DATE         0\n",
       "EXPIRE_DATE        0\n",
       "DTE                0\n",
       "C_DELTA            0\n",
       "C_GAMMA            0\n",
       "C_VEGA             0\n",
       "C_THETA            0\n",
       "C_RHO              0\n",
       "C_IV               0\n",
       "P_DELTA            0\n",
       "P_GAMMA            0\n",
       "P_VEGA             0\n",
       "P_THETA            0\n",
       "P_RHO              0\n",
       "P_IV               0\n",
       "STRIKE_DISTANCE    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using DataLoader to check for missing values in the DataFrame\n",
    "missing_values = DataLoader.check_missing_values(selected_data)\n",
    "\n",
    "# Displaying the number of missing values in each column\n",
    "missing_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35f32e",
   "metadata": {},
   "source": [
    "There are no missing values in our dataset. Every column has complete data, which simplifies the cleaning process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df546778",
   "metadata": {},
   "source": [
    "\n",
    "#### 2.4.2 Data Type Conversion\n",
    "\n",
    "-   Ensure that each column is of the appropriate data type for analysis. For example, date columns should have a date data type.\n",
    "\n",
    "Now, let's ensure that each column is of the appropriate data type for analysis. Specifically, we will check and possibly convert the `QUOTE_DATE` and `EXPIRE_DATE` columns to a date data type, which will facilitate any time-based analysis we might want to perform later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b4553c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QUOTE_DATE         datetime64[ns]\n",
       "EXPIRE_DATE        datetime64[ns]\n",
       "DTE                       float64\n",
       "C_DELTA                   float64\n",
       "C_GAMMA                   float64\n",
       "C_VEGA                    float64\n",
       "C_THETA                   float64\n",
       "C_RHO                     float64\n",
       "C_IV                       object\n",
       "P_DELTA                   float64\n",
       "P_GAMMA                   float64\n",
       "P_VEGA                    float64\n",
       "P_THETA                   float64\n",
       "P_RHO                     float64\n",
       "P_IV                       object\n",
       "STRIKE_DISTANCE           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns to convert to datetime data type\n",
    "columns_to_convert = ['QUOTE_DATE', 'EXPIRE_DATE']\n",
    "\n",
    "# Using DataLoader to convert specified columns to datetime data type\n",
    "converted_data = DataLoader.convert_to_datetime(selected_data.copy(), columns_to_convert)\n",
    "\n",
    "# Displaying data types of each column to verify the changes\n",
    "converted_data.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bc13ad",
   "metadata": {},
   "source": [
    "The `QUOTE_DATE` and `EXPIRE_DATE` columns have been successfully converted to datetime data types, which will be helpful for any time-based analyses we might want to conduct later.\n",
    "\n",
    "#### Observations:\n",
    "\n",
    "-   Datetime Conversion:\n",
    "    -   The `QUOTE_DATE` and `EXPIRE_DATE` columns are now in datetime format.\n",
    "-   Other Data Types:\n",
    "    -   Most other columns, such as the Greeks and `STRIKE_DISTANCE`, are in a numeric format (float64), which is suitable for mathematical operations and analyses.\n",
    "    -   However, the `C_IV` and `P_IV` (implied volatilities) columns are object types, likely due to some non-numeric values or inconsistencies in the data.\n",
    "\n",
    "#### Next Steps:\n",
    "\n",
    "-   We might want to further investigate and clean the `C_IV` and `P_IV` columns to ensure that they are in a numeric format suitable for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c7468f",
   "metadata": {},
   "source": [
    "#### 2.4.3 Descriptive Statistics\n",
    "\n",
    "-   Calculate basic descriptive statistics for the numeric columns to understand the distribution of values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3983d985",
   "metadata": {},
   "source": [
    "let's handle the `C_IV` and `P_IV` columns by doing the following:\n",
    "\n",
    "i.  Investigation:\n",
    "\n",
    "    -   Find out why these columns are not being recognized as numeric.\n",
    "    -   Check for any non-numeric or inconsistent values.\n",
    "ii.  Conversion:\n",
    "\n",
    "    -   We will try to convert these columns to a numeric data type, handling or removing any non-numeric values if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccd9d122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C_IV': array([' 4.346730', ' 3.872190', ' 3.682610', ..., ' 0.443730',\n",
       "        ' 0.430870', ' 0.287580'], dtype=object),\n",
       " 'P_IV': array([' 1.210050', ' 0.996160', ' 0.911990', ..., ' 0.144610',\n",
       "        ' 0.339160', ' 0.149600'], dtype=object)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns to check for unique values\n",
    "columns_to_check = ['C_IV', 'P_IV']\n",
    "\n",
    "# Using DataLoader to get unique values from specified columns\n",
    "unique_values = DataLoader.get_unique_values(converted_data, columns_to_check)\n",
    "\n",
    "# Displaying unique values to identify any inconsistencies\n",
    "unique_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068f63d8",
   "metadata": {},
   "source": [
    "It appears that the `C_IV` and `P_IV` columns are recognized as object data types because the values are being read as strings (text) rather than numeric values. This is likely due to the presence of spaces or other non-numeric characters.\n",
    "\n",
    "#### Solution:\n",
    "\n",
    "-   We can convert these columns to a numeric data type, which will make them more suitable for mathematical computations and analyses.\n",
    "-   Any non-numeric values or inconsistencies will be converted to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63b5aae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(QUOTE_DATE         datetime64[ns]\n",
       " EXPIRE_DATE        datetime64[ns]\n",
       " DTE                       float64\n",
       " C_DELTA                   float64\n",
       " C_GAMMA                   float64\n",
       " C_VEGA                    float64\n",
       " C_THETA                   float64\n",
       " C_RHO                     float64\n",
       " C_IV                      float64\n",
       " P_DELTA                   float64\n",
       " P_GAMMA                   float64\n",
       " P_VEGA                    float64\n",
       " P_THETA                   float64\n",
       " P_RHO                     float64\n",
       " P_IV                      float64\n",
       " STRIKE_DISTANCE           float64\n",
       " dtype: object,\n",
       " QUOTE_DATE             0\n",
       " EXPIRE_DATE            0\n",
       " DTE                    0\n",
       " C_DELTA                0\n",
       " C_GAMMA                0\n",
       " C_VEGA                 0\n",
       " C_THETA                0\n",
       " C_RHO                  0\n",
       " C_IV                7006\n",
       " P_DELTA                0\n",
       " P_GAMMA                0\n",
       " P_VEGA                 0\n",
       " P_THETA                0\n",
       " P_RHO                  0\n",
       " P_IV               29226\n",
       " STRIKE_DISTANCE        0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns to convert to numeric data type\n",
    "columns_to_convert_numeric = ['C_IV', 'P_IV']\n",
    "\n",
    "# Using DataLoader to convert specified columns to numeric data type\n",
    "numeric_converted_data = DataLoader.convert_to_numeric(converted_data.copy(), columns_to_convert_numeric)\n",
    "\n",
    "# Displaying data types of each column to verify the changes, and checking for any new missing values\n",
    "numeric_converted_data.dtypes, numeric_converted_data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5972b2",
   "metadata": {},
   "source": [
    "The `C_IV` and `P_IV` columns have been successfully converted to numeric data types. However, in the process, some values that couldn't be converted to numbers have become NaN (Not a Number). Here's a summary:\n",
    "\n",
    "##### i. Conversion to Numeric:\n",
    "-   The `C_IV` and `P_IV` columns are now of type float64, suitable for mathematical operations.\n",
    "\n",
    "##### ii. Handling NaN Values:\n",
    "-   There are 7,006 NaN values in the `C_IV` column and 29,226 in the `P_IV` column.\n",
    "-   These NaN values need to be addressed. We could either remove the rows with NaN values or fill them with a specific value, like the mean or median of the respective columns.\n",
    "\n",
    "##### Next Steps:\n",
    "-   We will decide on a strategy to handle the NaN values in the `C_IV` and `P_IV` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd801691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((209463, 16),\n",
       " QUOTE_DATE         0\n",
       " EXPIRE_DATE        0\n",
       " DTE                0\n",
       " C_DELTA            0\n",
       " C_GAMMA            0\n",
       " C_VEGA             0\n",
       " C_THETA            0\n",
       " C_RHO              0\n",
       " C_IV               0\n",
       " P_DELTA            0\n",
       " P_GAMMA            0\n",
       " P_VEGA             0\n",
       " P_THETA            0\n",
       " P_RHO              0\n",
       " P_IV               0\n",
       " STRIKE_DISTANCE    0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns to check for NaN values\n",
    "columns_to_check_na = ['C_IV', 'P_IV']\n",
    "\n",
    "# Using DataLoader to remove rows where specified columns have NaN values\n",
    "cleaned_data = DataLoader.remove_na_rows(numeric_converted_data.copy(), columns_to_check_na)\n",
    "\n",
    "# Displaying the shape of the DataFrame and checking for any remaining missing values\n",
    "cleaned_data.shape, cleaned_data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19faa1f",
   "metadata": {},
   "source": [
    "The rows containing NaN values in the `C_IV` and `P_IV` columns have been successfully removed. Now, our dataset is cleaner and more suitable for analysis.\n",
    "\n",
    "#### Summary of Changes:\n",
    "\n",
    "-   Rows Removed:\n",
    "    -   Rows with NaN values in the `C_IV` and `P_IV` columns were removed to maintain the accuracy of the implied volatility values.\n",
    "-   Current Dataset Shape:\n",
    "    -   The dataset now contains 209,463 rows and 16 columns, and there are no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80187af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 209463 entries, 0 to 245668\n",
      "Data columns (total 16 columns):\n",
      " #   Column           Non-Null Count   Dtype         \n",
      "---  ------           --------------   -----         \n",
      " 0   QUOTE_DATE       209463 non-null  datetime64[ns]\n",
      " 1   EXPIRE_DATE      209463 non-null  datetime64[ns]\n",
      " 2   DTE              209463 non-null  float64       \n",
      " 3   C_DELTA          209463 non-null  float64       \n",
      " 4   C_GAMMA          209463 non-null  float64       \n",
      " 5   C_VEGA           209463 non-null  float64       \n",
      " 6   C_THETA          209463 non-null  float64       \n",
      " 7   C_RHO            209463 non-null  float64       \n",
      " 8   C_IV             209463 non-null  float64       \n",
      " 9   P_DELTA          209463 non-null  float64       \n",
      " 10  P_GAMMA          209463 non-null  float64       \n",
      " 11  P_VEGA           209463 non-null  float64       \n",
      " 12  P_THETA          209463 non-null  float64       \n",
      " 13  P_RHO            209463 non-null  float64       \n",
      " 14  P_IV             209463 non-null  float64       \n",
      " 15  STRIKE_DISTANCE  209463 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(14)\n",
      "memory usage: 27.2 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'QUOTE_DATE': 62, 'EXPIRE_DATE': 92}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using DataLoader to display basic info of the DataFrame\n",
    "DataLoader.display_basic_info(cleaned_data)\n",
    "\n",
    "# Using DataLoader to display statistical summary of the DataFrame\n",
    "DataLoader.display_statistical_summary(cleaned_data)\n",
    "\n",
    "# Columns to display unique values\n",
    "columns_to_display_unique = ['QUOTE_DATE', 'EXPIRE_DATE']\n",
    "\n",
    "# Using DataLoader to display number of unique values in specified columns\n",
    "DataLoader.display_unique_values(cleaned_data, columns_to_display_unique)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895d891e",
   "metadata": {},
   "source": [
    "### 2.5: Data Summary\n",
    "\n",
    "#### Data Acquisition:\n",
    "\n",
    "-   The dataset consists of options data related to SPY (S&P 500 ETF), which seems to have been obtained from text files. Since, I am very much interested in trading options, I dived into the research of acquiring the options data for Project 1 - EDA. We acquired this data from https://www.optionsdx.com/. Optionsdx provides free Historical Options Data. This particular Options data belons to SPY ETF. SPY is American Options meaning option holders can exercise their option at any time before expiration. In the liquid market, SPY is one of the most heavily traded ETFs and is the oldest ETF still trading.\n",
    "\n",
    "#### Use Cases and Attributes:\n",
    "\n",
    "-   The cleaned dataset provides 209,463 use cases (rows), each representing an option data point with various attributes such as Greeks, implied volatilities, and date-related information.\n",
    "-   Each use case has 16 attributes, including Greeks ('C_DELTA', 'C_GAMMA', 'C_VEGA', 'C_THETA', 'C_RHO', 'P_DELTA', 'P_GAMMA', 'P_VEGA', 'P_THETA', 'P_RHO'), implied volatilities ('C_IV', 'P_IV'), and date-related information ('QUOTE_DATE', 'EXPIRE_DATE', 'DTE', 'STRIKE_DISTANCE').\n",
    "\n",
    "#### Data Types:\n",
    "\n",
    "-   Columns like 'QUOTE_DATE' and 'EXPIRE_DATE' are datetime types, providing precise date information.\n",
    "-   Greeks and implied volatilities are float types, which are suitable for mathematical analysis and model building."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faa472d",
   "metadata": {},
   "source": [
    "### 2.6 Saving Cleaned Data\n",
    "We will save the cleaned file as 'cleaned_data.csv' file for backup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d93ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path to save the cleaned data\n",
    "file_path_to_save = 'cleaned_data.csv'\n",
    "\n",
    "# Using DataLoader to save the cleaned DataFrame to a CSV file\n",
    "DataLoader.save_to_csv(cleaned_data, file_path_to_save)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
